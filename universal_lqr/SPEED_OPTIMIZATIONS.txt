SPEED OPTIMIZATIONS FOR MULTI-GPU TRAINING
===========================================

CHANGES MADE TO train_fast.py:
===============================

1. INCREASED NUM_WORKERS (CRITICAL FIX)
   -------------------------------------
   OLD: 4 workers
   NEW: 24 workers (8 per GPU × 3 GPUs)
   
   Why: With 4 workers and 3 GPUs, the GPUs were starving for data!
   Each GPU needs a steady stream of batches. With 24 workers, each
   worker can prefetch batches while GPUs are computing.


2. ADDED PREFETCH_FACTOR
   ----------------------
   NEW: prefetch_factor=4
   
   Why: Each of the 24 workers now prefetches 4 batches ahead.
   This means 96 batches are being loaded in parallel while GPUs
   are computing. Eliminates data loading bottleneck.


3. OPTIMIZED HDF5 READING
   -----------------------
   Added cache configuration:
   - rdcc_nbytes=1GB per worker
   - rdcc_nslots=10000 chunk slots
   
   Why: HDF5 now caches frequently accessed data in RAM per worker.
   Dramatically reduces disk I/O.


4. ADDED PROGRESS MONITORING
   --------------------------
   Progress bar now shows:
   - Current batch loss
   - Running average loss
   
   Why: You can see if training is progressing without waiting
   for epoch to finish.


WHAT TO DO ON YOUR SERVER:
===========================

Step 1: Transfer updated files
-------------------------------
From your Mac, run:

scp /Users/turki/Desktop/My_PhD/highway_merging/ablation/universal_lqr/train_fast.py \
    maitham@ube:~/train_lqr/universal_lqr/

scp /Users/turki/Desktop/My_PhD/highway_merging/ablation/universal_lqr/diagnose_speed.py \
    maitham@ube:~/train_lqr/universal_lqr/


Step 2: Run diagnostics (RECOMMENDED)
--------------------------------------
SSH to server:
ssh maitham@ube
cd ~/train_lqr/universal_lqr

Run diagnostic:
python3 diagnose_speed.py

This will:
- Test different numbers of workers
- Measure GPU computation speed
- Test single vs multi-GPU
- Estimate total training time
- Give you specific recommendations


Step 3: Train with optimized settings
--------------------------------------
python3 train_fast.py 2>&1 | tee training.log

You should now see:
- DataLoader config: num_workers=24, prefetch_factor=4
- Progress bar updating with loss
- Much faster batches/sec


EXPECTED IMPROVEMENTS:
======================

Before optimization:
- 4 workers, no prefetch, no HDF5 cache
- GPUs waiting for data (low utilization)
- ~15-30 minutes per epoch (slow!)

After optimization:
- 24 workers, prefetch=4, 1GB HDF5 cache per worker
- GPUs at 95-100% utilization
- ~3-5 minutes per epoch (FAST!)
- Total training: 1.5-2.5 hours for 30 epochs


MONITORING GPU UTILIZATION:
============================

In a separate terminal, run:
watch -n 0.5 nvidia-smi

You should see:
- All 3 GPUs at 90-100% utilization
- Memory usage ~9-10 GB per GPU
- GPU temperature rising (60-80°C is normal)

If GPUs are below 80% utilization:
- Data loading is still a bottleneck
- Try copying HDF5 file to RAM disk (see below)


ADVANCED OPTIMIZATION (if still slow):
=======================================

1. Copy HDF5 to RAM disk for fastest I/O:
   ---------------------------------------
   # Create RAM disk (requires sudo)
   sudo mkdir /dev/shm/train_data
   sudo cp ~/train_lqr/universal_lqr/data/training_data.h5 /dev/shm/train_data/
   
   # Edit train_fast.py to use this path:
   h5_file = '/dev/shm/train_data/training_data.h5'
   
   This eliminates ALL disk I/O! Data loads from RAM at 50+ GB/s.


2. Increase batch size:
   --------------------
   Edit config.py:
   TRAINING_CONFIG['batch_size'] = 2048  # Up from 1024
   
   With 3 GPUs: 2048 × 3 = 6144 effective batch size
   May need to reduce learning_rate to 1e-4.


3. Reduce dataset size (if you just want quick training):
   -------------------------------------------------------
   Edit config.py:
   SEQUENCE_STRIDE = 32  # Up from 1
   
   Then regenerate data:
   python3 data_generation.py
   
   This creates 2.3M sequences instead of 72M.
   Training will take ~40 minutes for 30 epochs.


TROUBLESHOOTING:
================

Issue: "Too many open files"
Fix: ulimit -n 65536

Issue: "RuntimeError: CUDA out of memory"
Fix: Reduce batch_size to 512 in config.py

Issue: Still slow after optimization
Check: Run diagnose_speed.py to identify bottleneck
       If data loading is slow, use RAM disk method above

Issue: GPUs not at 100%
Check: CPU usage with 'htop' - if CPUs are at 100%, need more workers
       Disk I/O with 'iostat -x 1' - if high, use RAM disk


NEXT STEPS:
===========

1. Run diagnose_speed.py to verify optimizations worked
2. Start training with optimized train_fast.py
3. Monitor with nvidia-smi to confirm GPU utilization
4. Training should complete in 1.5-2.5 hours
5. Use test_jax.py (or create test_pytorch.py) to evaluate model

