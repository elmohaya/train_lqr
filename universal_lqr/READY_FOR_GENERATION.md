# âœ… Ready for Data Generation!

## Summary of All Changes

### ðŸŽ¯ Main Achievement: Per-System Normalization

**Problem Solved**: Different LTI systems have different physical units. Normalizing them together was wrong.

**Solution**: Each system is normalized by its own maximum values encountered during simulation.

## Changes Made

### 1. R Matrix Fixes (Control Penalties)
- âœ… Increased R matrices by 10-100x across all 36 systems
- âœ… Fixed systems with huge control signals (InvertedPendulum, SegwayRobot, etc.)
- âœ… Results: Max control reduced from 1.15 MILLION to < 25,000

### 2. Per-System Normalization (`data_generation_jax.py`)
- âœ… Track system_id for every sequence
- âœ… Track actual max |x| and max |u| per system during generation
- âœ… After generation: normalize by system-specific maxes
- âœ… Save normalization factors to HDF5 for later denormalization
- âœ… Increased parameter uncertainty: Â±10% â†’ Â±20%

### 3. Data Loader Update (`data_loader.py`)
- âœ… Removed active normalization (data already normalized)
- âœ… Added documentation explaining the new approach
- âœ… Backward compatible (dummy mean=0, std=1 stored)

### 4. Updated R Matrices in All Systems
**Files modified**:
- `systems/mechanical_systems.py`
- `systems/robotics_systems.py`
- `systems/aerospace_systems.py`
- `systems/vehicle_systems.py`
- `systems/electrical_systems.py`
- `systems/other_systems.py`

### 5. Debug and Documentation
- âœ… `data_generation_debug.py` - visualization tool with improved plots
- âœ… `PER_SYSTEM_NORMALIZATION.md` - detailed documentation
- âœ… `READY_FOR_GENERATION.md` - this file!
- âœ… `GREAT_LAKES_SETUP.md` - GPU training guide

## What's in the Generated HDF5 File

```
training_data_jax.h5
â”œâ”€â”€ input_sequences         # (N, 32, 19) - States + encoding, NORMALIZED
â”œâ”€â”€ controls                # (N, 6) - Control signals, NORMALIZED  
â”œâ”€â”€ control_masks           # (N, 6) - Which controls are active
â”œâ”€â”€ system_ids              # (N,) - System name for each sequence
â”œâ”€â”€ normalization/          # â­ NEW: Per-system max values
â”‚   â”œâ”€â”€ InvertedPendulum/
â”‚   â”‚   â”œâ”€â”€ state_max       # [Î¸_max, Î¸_dot_max]
â”‚   â”‚   â”œâ”€â”€ control_max     # [u_max]
â”‚   â”‚   â””â”€â”€ attrs: n_states=2, n_inputs=1
â”‚   â”œâ”€â”€ CartPole/
â”‚   â”‚   â”œâ”€â”€ state_max       # [x_max, x_dot_max, Î¸_max, Î¸_dot_max]
â”‚   â”‚   â”œâ”€â”€ control_max     # [F_max]
â”‚   â”‚   â””â”€â”€ attrs: n_states=4, n_inputs=1
â”‚   â””â”€â”€ ... (35 systems total)
â””â”€â”€ attrs: total_sequences, parameter_uncertainty=0.20, etc.
```

## How to Run Data Generation

```bash
cd /Users/turki/Desktop/My_PhD/highway_merging/ablation/universal_lqr/jax_implementation

# Generate data (will take time!)
python data_generation_jax.py
```

**Expected output**:
- File: `data/training_data_jax.h5`
- Size: ~50-100 GB (for 100M samples)
- Time: Several hours (depends on CPU/GPU)

## What Happens During Generation

1. **For each system** (35 systems):
   - Create 50 variants (Â±20% parameter uncertainty)
   - Generate 500 trajectories per variant
   - Total: 25,000 trajectories per system

2. **Track max values**:
   - Record max |x_i| for each state dimension
   - Record max |u_j| for each control dimension
   - Per system, not global!

3. **Generate sequences**:
   - Extract sliding windows from trajectories
   - Store raw (unnormalized) data initially

4. **Normalize**:
   - After ALL data generated
   - Divide each system's data by its own maxes
   - `x_norm = x_raw / x_max_for_system`
   - `u_norm = u_raw / u_max_for_system`

5. **Save normalization factors**:
   - Store max values in HDF5
   - Needed to denormalize transformer outputs

## Using Normalized Data for Training

### Training
```python
# Data is already normalized - just load and train!
from data_loader import JAXDataLoader

train_loader = JAXDataLoader(
    'data/training_data_jax.h5',
    batch_size=512,
    shuffle=True,
    to_jax=True
)

for batch in train_loader:
    # batch['input_sequences'] is normalized
    # batch['controls'] is normalized
    predictions = model(batch['input_sequences'])
    loss = mse_loss(predictions, batch['controls'], batch['control_masks'])
```

### Inference (Denormalization)
```python
import h5py

# Load normalization factors
with h5py.File('data/training_data_jax.h5', 'r') as f:
    system_name = 'InvertedPendulum'
    state_max = f[f'normalization/{system_name}/state_max'][:]
    control_max = f[f'normalization/{system_name}/control_max'][:]

# Normalize input
x_raw = np.array([0.1, 0.5])  # [angle, angular_velocity]
x_norm = x_raw / state_max

# Get transformer prediction
u_norm = transformer.predict(x_norm_sequence)

# Denormalize output
u_raw = u_norm * control_max
print(f"Actual control: {u_raw} NÂ·m")
```

## Verification After Generation

```python
import h5py
import numpy as np

with h5py.File('data/training_data_jax.h5', 'r') as f:
    # Check data is normalized
    inputs = f['input_sequences'][:1000]
    controls = f['controls'][:1000]
    
    print(f"Input range: [{np.min(inputs):.2f}, {np.max(inputs):.2f}]")
    print(f"Control range: [{np.min(controls):.2f}, {np.max(controls):.2f}]")
    # Expected: roughly [-1, 1] with some outliers
    
    # Check normalization factors exist
    print(f"\nSystems with normalization factors:")
    for sys_name in list(f['normalization'].keys())[:5]:
        state_max = f[f'normalization/{sys_name}/state_max'][:]
        control_max = f[f'normalization/{sys_name}/control_max'][:]
        n_states = f[f'normalization/{sys_name}'].attrs['n_states']
        n_inputs = f[f'normalization/{sys_name}'].attrs['n_inputs']
        print(f"  {sys_name}: {n_states} states, {n_inputs} inputs")
        print(f"    state_max: {state_max}")
        print(f"    control_max: {control_max}")
```

## Expected Benefits

1. âœ… **Proper Unit Handling**: Each system normalized by its own scale
2. âœ… **Equal Contribution**: All systems contribute equally to training
3. âœ… **No Unit Mixing**: Angles (rad) separate from positions (m)
4. âœ… **Better Control Signals**: No more million-scale values!
5. âœ… **Easy Deployment**: Save max values for denormalization

## Training on Great Lakes

See `GREAT_LAKES_SETUP.md` for:
- JAX GPU installation
- SLURM script (`train_gpu.slurm`)
- Expected training time: **3-4 hours on V100 GPU** (vs 2 days on M4 CPU)

## Next Steps

1. **Generate Data**: Run `python data_generation_jax.py`
2. **Verify Data**: Check normalization factors are saved
3. **Train on GPU**: Use Great Lakes for 30-60x speedup
4. **Monitor Loss**: Should start at 5-10 (not 18,000!)
5. **Deploy**: Use normalization factors to denormalize predictions

---

**Status**: ðŸŸ¢ **READY TO GENERATE DATA!** All code modifications complete.

**Key Files**:
- `data_generation_jax.py` - Main generation script (MODIFIED âœ…)
- `data_loader.py` - Data loader (MODIFIED âœ…)
- `systems/*.py` - All 36 systems with fixed R matrices (MODIFIED âœ…)
- `train_jax.py` - Training script (READY âœ…)
- `GREAT_LAKES_SETUP.md` - GPU training guide (READY âœ…)

